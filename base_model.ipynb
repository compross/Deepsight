{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPAYOGKrpxlOxkAyHw+PsDM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MaEUroPhpsMH","executionInfo":{"status":"ok","timestamp":1733348988783,"user_tz":360,"elapsed":18959,"user":{"displayName":"Compton Ross","userId":"13306003155380130426"}},"outputId":"e423af23-12ad-43a1-9b55-7389d53e9d7e"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","\n","\n","df = pd.read_csv('/content/drive/MyDrive/543 Project/clinvar.csv')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7REGFN83p30n","executionInfo":{"status":"ok","timestamp":1731939817358,"user_tz":360,"elapsed":5625,"user":{"displayName":"Compton Ross","userId":"13306003155380130426"}},"outputId":"12a50acf-c46b-4887-b239-2471de4a4558"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-2-d44202e0ad81>:4: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n","  df = pd.read_csv('/content/drive/MyDrive/543 Project/cleaned_clinvar.csv')  # Replace with your actual file path\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.preprocessing import OneHotEncoder\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score"],"metadata":{"id":"AX0isS8FsHHC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def encode_allele(allele):\n","    mapping = {'A': [1, 0, 0, 0],\n","               'C': [0, 1, 0, 0],\n","               'G': [0, 0, 1, 0],\n","               'T': [0, 0, 0, 1]}\n","    return mapping.get(allele, [0, 0, 0, 0])\n","\n","df['ref_encoded'] = df['ref'].apply(encode_allele)\n","df['alt_encoded'] = df['alt'].apply(encode_allele)"],"metadata":{"id":"668pN72HsMUx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df['chrom'] = df['chrom'].astype(str)\n","\n","chrom_encoder = OneHotEncoder(sparse_output=False)\n","chrom_encoded = chrom_encoder.fit_transform(df[['chrom']])\n","\n","df['chrom_encoded'] = list(chrom_encoded)"],"metadata":{"id":"bhfdDg_4sQ4z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#normalize position col\n","df['pos_normalized'] = df['pos'] / df['pos'].max()"],"metadata":{"id":"5LvfhoVHsTfX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def combine_features(row):\n","    features = []\n","    features.extend(row['ref_encoded'])\n","    features.extend(row['alt_encoded'])\n","    features.extend(row['chrom_encoded'])\n","    features.append(row['pos_normalized'])\n","    return features\n","\n","\n","df['combined_features'] = df.apply(combine_features, axis=1)\n","df['label'] = df['label'].astype(np.float32)"],"metadata":{"id":"BEJ2MeOVsVzh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["features = df['combined_features'].tolist()\n","labels = df['label'].values\n","\n","features = np.array(features, dtype=np.float32)\n","labels = np.array(labels, dtype=np.float32)\n","\n"],"metadata":{"id":"7YyrzBEesZKK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","X_train, X_test, y_train, y_test = train_test_split(\n","    features, labels, test_size=0.2, random_state=42\n",")\n"],"metadata":{"id":"05DmoWPrseDt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class VariantDataset(Dataset):\n","    def __init__(self, features, labels):\n","        self.features = torch.tensor(features, dtype=torch.float32)\n","        self.labels = torch.tensor(labels, dtype=torch.float32)\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","    def __getitem__(self, idx):\n","        feature = self.features[idx]\n","        label = self.labels[idx]\n","        return feature, label\n"],"metadata":{"id":"Ud0SEriYsgsq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_dataset = VariantDataset(X_train, y_train)\n","test_dataset = VariantDataset(X_test, y_test)\n","\n","batch_size = 32\n","\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size)\n"],"metadata":{"id":"9oy6NVc_sio5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","input_dim = X_train.shape[1]\n","\n","class SimpleClassifier(nn.Module):\n","    def __init__(self, input_dim):\n","        super(SimpleClassifier, self).__init__()\n","        self.layers = nn.Sequential(\n","            nn.Linear(input_dim, 64),\n","            nn.ReLU(),\n","            nn.Dropout(0.5),\n","            nn.Linear(64, 32),\n","            nn.ReLU(),\n","            nn.Dropout(0.5),\n","            nn.Linear(32, 1),\n","            nn.Sigmoid()\n","        )\n","\n","    def forward(self, x):\n","        return self.layers(x)\n","\n","\n","model = SimpleClassifier(input_dim)"],"metadata":{"id":"Lnep57TUslgU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#binary cross entropy loss\n","criterion = nn.BCELoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n"],"metadata":{"id":"mijmNqsLsqaG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["epochs = 10\n","\n","for epoch in range(epochs):\n","    model.train()\n","    total_loss = 0\n","    for batch_features, batch_labels in train_loader:\n","        optimizer.zero_grad()\n","        outputs = model(batch_features)\n","        outputs = outputs.squeeze()\n","        batch_labels = batch_labels.squeeze()\n","        loss = criterion(outputs, batch_labels)\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","    avg_loss = total_loss / len(train_loader)\n","    print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AoqkspYMssbM","executionInfo":{"status":"ok","timestamp":1731941600272,"user_tz":360,"elapsed":745554,"user":{"displayName":"Compton Ross","userId":"13306003155380130426"}},"outputId":"b1059f65-8799-49dd-ec73-e5dbcacdd0f5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10, Loss: 0.4261\n","Epoch 2/10, Loss: 0.4227\n","Epoch 3/10, Loss: 0.4221\n","Epoch 4/10, Loss: 0.4219\n","Epoch 5/10, Loss: 0.4215\n","Epoch 6/10, Loss: 0.4215\n","Epoch 7/10, Loss: 0.4212\n","Epoch 8/10, Loss: 0.4212\n","Epoch 9/10, Loss: 0.4212\n","Epoch 10/10, Loss: 0.4208\n"]}]},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","\n","model.eval()\n","with torch.no_grad():\n","    all_preds = []\n","    all_labels = []\n","    for batch_features, batch_labels in test_loader:\n","        outputs = model(batch_features)\n","        predicted = (outputs.squeeze() >= 0.5).float()\n","        all_preds.extend(predicted.numpy())\n","        all_labels.extend(batch_labels.numpy())\n","\n","\n","    accuracy = accuracy_score(all_labels, all_preds)\n","\n","    precision = precision_score(all_labels, all_preds, zero_division=0)\n","    recall = recall_score(all_labels, all_preds, zero_division=0)\n","    f1 = f1_score(all_labels, all_preds, zero_division=0)\n","\n","    print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n","    print(f\"Precision: {precision * 100:.2f}%\")\n","    print(f\"Recall: {recall * 100:.2f}%\")\n","    print(f\"F1 Score: {f1 * 100:.2f}%\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GTw3LDELsvTX","executionInfo":{"status":"ok","timestamp":1731941803705,"user_tz":360,"elapsed":9220,"user":{"displayName":"Compton Ross","userId":"13306003155380130426"}},"outputId":"65efe9a0-dd9f-4a96-987c-bdde5ba82039"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Test Accuracy: 84.64%\n","Precision: 86.09%\n","Recall: 96.80%\n","F1 Score: 91.13%\n"]}]},{"cell_type":"markdown","source":["For predicting with new variants\n"],"metadata":{"id":"Y76MdrY2s3JW"}},{"cell_type":"code","source":["new_data = pd.DataFrame({\n","    'chrom': ['1', '2'],\n","    'pos': [123456, 234567],\n","    'ref': ['A', 'G'],\n","    'alt': ['G', 'C']\n","})\n","\n","#preprocess data\n","new_data['ref_encoded'] = new_data['ref'].apply(encode_allele)\n","new_data['alt_encoded'] = new_data['alt'].apply(encode_allele)\n","new_chrom_encoded = chrom_encoder.transform(new_data[['chrom']])\n","new_data['chrom_encoded'] = list(new_chrom_encoded)\n","new_data['pos_normalized'] = new_data['pos'] / df['pos'].max()\n","new_data['combined_features'] = new_data.apply(combine_features, axis=1)\n","\n","new_features = np.array(new_data['combined_features'].tolist(), dtype=np.float32)\n","new_features = torch.tensor(new_features, dtype=torch.float32)\n","\n","\n","model.eval()\n","with torch.no_grad():\n","    outputs = model(new_features)\n","    probabilities = outputs.squeeze().numpy()\n","    predictions = (probabilities >= 0.5).astype(int)\n","    print(\"New Data Predictions (Probabilities):\", probabilities)\n","    print(\"New Data Predictions (Classes):\", predictions)"],"metadata":{"id":"WEFqd11js5D7"},"execution_count":null,"outputs":[]}]}